{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd7dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da097d59",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8190f8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForMaskedLM\n",
    ")\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6b0776",
   "metadata": {},
   "source": [
    "### Ideas\n",
    "\n",
    "- Try a BERTLM from transformer\n",
    "- Can you separate the encoder and decoder?\n",
    "- Can we show that the embeddings from BERT have the same pattern as GLOVE?\n",
    "    - Generate word vectors for vocabulary in glove embeddings\n",
    "    - Get gender biased words\n",
    "    - Get Null Projection\n",
    "    - Plot diagrams showing cluster change after null projection\n",
    "    - Acquire other polarised words, hot-cold, up-down, soft-hard, nature-manmade, etc\n",
    "    - Should that the clusters for these words are still separatable after null projection\n",
    "- Can we use the language model encoder / decoder to apply null projection inbetween to should change in output with projection?\n",
    "    - e.g. input -> tokeniser -> encoder -> (null projection OR identity) -> decoder -> tokeniser -> output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffd356e",
   "metadata": {},
   "source": [
    "# BERT Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7aca71aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c0123f1e6c412fb8b9fddbf8fa7e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5421d6f8a2c24dd59d7c31303eb9033d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ecc1b02989849b4b1010c7d3cbe820d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6becfe5dbbdd47f799ed8367afb605a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "705d4a5b11f543079afd4d5db3385c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_mlm = BertForMaskedLM.from_pretrained('bert-base-uncased', return_dict = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a6d155",
   "metadata": {},
   "source": [
    "![title](images/bert_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add7e792",
   "metadata": {},
   "source": [
    "Source: [**BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova](https://arxiv.org/abs/1810.04805)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ac0bc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(input_list, chunk_size):\n",
    "    for i in range(0, len(input_list), chunk_size):\n",
    "        yield input_list[i:i + chunk_size]\n",
    "        \n",
    "def get_embeddings(input_sequences, model, tokenizer):\n",
    "    tokenized_input = bert_tokenizer.batch_encode_plus(input_sequences, return_tensors = \"pt\", padding=True, truncation=False)\n",
    "    embeddings = bert_mlm(**tokenized_input, output_hidden_states=True).hidden_states[BERT_BIAS_LAYER]\n",
    "    return embeddings.detach().numpy(), tokenized_input[\"input_ids\"].detach().numpy()\n",
    "\n",
    "def extract_token_embeddings(embeddings, input_ids):\n",
    "    extracted_embeddings = []\n",
    "    for idx in range(embeddings.shape[0]):\n",
    "        if 0 in input_ids[idx]: # if input contains padding\n",
    "            eos_idx = list(input_ids[idx]).index(0) - 1\n",
    "        else:\n",
    "            eos_idx = list(input_ids[idx]).index(102)\n",
    "        extracted_embeddings.append(embeddings[idx][1:eos_idx].mean(axis=0))\n",
    "    return np.array(extracted_embeddings)\n",
    "\n",
    "def guard_vector(layer):\n",
    "    return NULL_PROJECTION.dot(layer.T).T\n",
    "\n",
    "def guard_embedding(hidden_state, tokenized_input):    \n",
    "    input_ids_numpy = list(tokenized_input[\"input_ids\"].detach().numpy()[0])\n",
    "    word_indexes = [input_ids_numpy.index(token_id) for token_id in input_ids_numpy if token_id not in [101, 103, 102, 0]]\n",
    "    bias_layer_numpy =  hidden_state.detach().numpy()\n",
    "    for idx in word_indexes:\n",
    "        bias_layer_numpy[0][idx] = guard_vector(bias_layer_numpy[0][idx])\n",
    "    return torch.Tensor(bias_layer_numpy)\n",
    "\n",
    "def run_post_bias_encoder_layers(encoder_layers_list, previous_hidden_state):\n",
    "    for attention_block in encoder_layers_list:\n",
    "        previous_hidden_state = attention_block.forward(hidden_states=previous_hidden_state)[0]\n",
    "    return previous_hidden_state\n",
    "\n",
    "def get_next_word(logits, tokenizer, mask_index):\n",
    "    softmax = F.softmax(logits, dim = -1)\n",
    "    mask_word = softmax[0, mask_index, :]\n",
    "    top_word = torch.argmax(mask_word, dim=1)\n",
    "    return tokenizer.decode(top_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c31abe",
   "metadata": {},
   "source": [
    "# BERT Vector Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de9527c",
   "metadata": {},
   "source": [
    "![title](images/bert_layers_gender_bias.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee43de6",
   "metadata": {},
   "source": [
    "Source: [**Investigating Gender Bias in BERT**: Rishabh Bhardwaj, Navonil Majumder, Soujanya Poria](https://arxiv.org/abs/2009.05021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be80ba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_BIAS_LAYER = 1\n",
    "VOCABULARY = pd.read_csv(\"data/vocabulary.txt\", header=None)[0].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ffad7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [02:32, 19.07s/it]\n"
     ]
    }
   ],
   "source": [
    "bert_vocab_embedding_list = np.empty((0, 768))\n",
    "for chunk in tqdm(chunker(VOCABULARY, 1000)):\n",
    "    embeddings, input_ids = get_embeddings(chunk, bert_mlm, bert_tokenizer)\n",
    "    embeddings = extract_token_embeddings(embeddings, input_ids)\n",
    "    bert_vocab_embedding_list = np.concatenate((bert_vocab_embedding_list, embeddings), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57cc7d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_shape = bert_vocab_embedding_list.shape\n",
    "with open(\"data/embeddings/BERTLM_ENCODER_LAYER_ONE/bert-base-uncased-embeddings.txt\", \"w\") as bert_file:\n",
    "    bert_file.write(f\"{embedding_shape[0]} {embedding_shape[1]} \\n\")\n",
    "    for word, embedding in zip(VOCABULARY, bert_vocab_embedding_list):\n",
    "        bert_file.write(f\"{word} {' '.join(map(str, list(embedding)))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17790dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pca explained variance ratio: [0.32702196 0.20407492 0.15900126 0.08174839 0.05758673 0.05069628\n",
      " 0.04543994 0.03472261 0.02933717 0.01037071] \n",
      "\n",
      "TOP 100 MALE SENSITIVE TOKENS \n",
      " ('man', 'john', 'he', 'boy', 'guy', 'son', 'his', 'manhunt', 'him', 'housman', 'himself', 'heisman', 'manu', 'johny', 'heyman', 'sons', 'hes', 'guymon', 'sonnen', 'boye', 'brothers', 'rockman', 'mangini', 'father', 'brother', 'mike', 'sandman', 'bossman', 'heder', 'jono', 'helt', 'hedi', 'hegel', 'dude', 'bluesman', 'jason', 'mr', 'sonali', 'redman', 'sonographer', 'linesman', 'charles', 'cashman', 'handsome', 'boyhood', 'ferdinando', 'marksman', 'darkman', 'timo', 'kingdon', 'cocky', 'walter', 'sonja', 'james', 'robert', 'heba', 'matthew', 'irishman', 'pitman', 'martino', 'paulus', 'mikel', 'juwan', 'tradesman', 'ockham', 'welshman', 'bagman', 'edgar', 'hegelian', 'vorderman', 'richard', 'william', 'david', 'countryman', 'bouchon', 'kings', 'waltrip', 'rotman', 'samo', 'heliotrope', 'wilfredo', 'cheeseman', 'thomas', 'lovano', 'gruff', 'horacio', 'jon', 'steven', 'robo', 'henry', 'guys', 'musicman', 'lad', 'soundman', 'rifleman', 'adam', 'henman', 'theophilus', 'jimbo', 'josephus')\n",
      "\n",
      "\n",
      "TOP 100 FEMALE SENSITIVE TOKENS \n",
      " ('mary', 'she', 'woman', 'shelia', 'women', 'feminism', 'feminist', 'actresses', 'lesbian', 'sheree', 'herself', 'goddess', 'aunt', 'actress', 'flowergirl', 'mother', 'grandmother', 'marylou', 'female', 'sherrie', 'her', 'heroine', 'galina', 'nursing', 'empress', 'maternity', 'mothers', 'sisters', 'sherri', 'daughters', 'cowgirl', 'sorority', 'brunette', 'hostess', 'countess', 'edith', 'gal', 'pregnant', 'wta', 'nurses', 'girl', 'abbess', 'granddaughter', 'daughter', 'anabelle', 'sister', 'priestess', 'girls', 'niece', 'duchess', 'maryse', 'childbirth', 'lady', 'hers', 'mildred', 'abby', 'superwoman', 'auntie', 'nurse', 'katie', 'feministe', 'nicolette', 'womans', 'maggie', 'louise', 'marjorie', 'freyja', 'lucinda', 'womanly', 'feminine', 'saleswoman', 'waitress', 'katerina', 'schoolgirl', 'homegirl', 'mistress', 'sheff', 'dowager', 'suffragette', 'womanhood', 'princess', 'supergirl', 'elisabeth', 'agnes', 'camgirl', 'feministing', 'grandma', 'katina', 'womens', 'frontwoman', 'widowed', 'feminists', 'diva', 'aunty', 'evelina', 'whore', 'irina', 'josie', 'gloriana', 'jenna')\n",
      "\n",
      "\n",
      "TOP 100 NEUTRAL TOKENS \n",
      " ('kundli', 'kirshner', 'cern', 'pietro', 'katekyo', 'argerich', 'emerita', 'bahnhof', 'game', 'pelagia', 'abang', 'itec', 'wknd', 'gus', 'hélène', 'destabilise', 'coloratura', 'kings', 'webdev', 'covergirl', 'ogura', 'jewelries', 'matt', 'larissa', 'nietzsche', 'pankaj', 'suvari', 'organdy', 'occurances', 'anfield', 'deist', 'birefringent', 'joana', 'pesaro', 'idiota', 'eku', 'eleanore', 'anabolics', 'thornberry', 'phantasmagoria', 'krystal', 'leyland', 'premarin', 'reviewposted', 'rivals', 'parler', 'seigneur', 'casteel', 'elissa', 'carranza', 'maureen', 'dinara', 'midori', 'townshend', 'minnillo', 'kapadia', 'strout', 'schoolteacher', 'airwolf', 'mami', 'katherine', 'daughters', 'roach', 'mariah', 'mankind', 'parshall', 'canibus', 'deregulate', 'isempty', 'pistons', 'jonah', 'boy', 'openmp', 'hibbert', 'nonconductive', 'lucy', 'callisto', 'horacio', 'tmn', 'salma', 'pabx', 'chandrasekhar', 'influential', 'barby', 'adela', 'cicero', 'taurasi', 'vigneault', 'peals', 'yhu', 'xavi', 'christine', 'relativity', 'userland', 'chickipedia', 'symone', 'beyonce', 'calligraphers', 'gliac', 'sdv')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python get_bias_sensitive_tokens.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92353406",
   "metadata": {},
   "source": [
    "# Null-Space Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1ca7e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 147; Dev size: 63; Test size: 90\n",
      "iteration: 24, accuracy: 0.36507936507936506: 100%|█| 25/25 [00:07<00:00,  3.25i\n",
      "Figure(600x500)\n",
      "Figure(600x500)\n",
      "V-measure-before (TSNE space): 0.778190793392485\n",
      "V-measure-after (TSNE space): 0.0011550932483761207\n",
      "V-measure-before (original space): 1.0\n",
      "V-measure-after (original space): 0.0007205831499929152\n"
     ]
    }
   ],
   "source": [
    "!python context_nullspace_projection.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a4ead3",
   "metadata": {},
   "source": [
    "![title](images/tsne_projections.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a8e4bf",
   "metadata": {},
   "source": [
    "# Transformer Encoder / Decoder Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1767d7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NULL_PROJECTION = np.load(\"data/nullspace_vector.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57e6ee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_word(input_sequence, model, tokenizer, guard_flag=False, biased_layer_index=1):  \n",
    "    # extracting modules from BERT LM\n",
    "    bert_encoder_modules = list(bert_mlm.modules())[8:-5] # extract list of model components\n",
    "    encoder_layers_list = [bert_encoder_modules[idx] for idx in range(19, 206, 17)] # extracting each encoder attention block\n",
    "    bert_mlm_head = bert_encoder_modules[-1] # extracting BERT LM Head\n",
    "\n",
    "    # tokenize input sequence\n",
    "    tokenized_input = tokenizer.encode_plus(input_sequence, return_tensors = \"pt\")\n",
    "    mask_index = torch.where(tokenized_input[\"input_ids\"][0] == bert_tokenizer.mask_token_id)\n",
    "\n",
    "    # extracting encoding and feeding back into model\n",
    "    hidden_state = model(**tokenized_input, output_hidden_states=True).hidden_states[biased_layer_index]\n",
    "    \n",
    "    # apply guarding function to hidden state\n",
    "    hidden_state = guard_embedding(hidden_state, tokenized_input) if guard_flag else hidden_state\n",
    "    \n",
    "    # run guarded hidden state through remaining encoder layers\n",
    "    encoder_output = run_post_bias_encoder_layers(encoder_layers_list, hidden_state)\n",
    "    \n",
    "    # pass encoder output into LM Head to generate logits\n",
    "    output_logits = bert_mlm_head.forward(sequence_output=encoder_output)\n",
    "\n",
    "    # generate the highest likelihood word\n",
    "    return get_next_word(output_logits, tokenizer, mask_index)\n",
    "\n",
    "def complete_the_sentence(female_variant, male_variant):\n",
    "    guarded_pred_female = generate_next_word(female_variant, bert_mlm, bert_tokenizer, guard_flag=True)\n",
    "    unguarded_pred_female = generate_next_word(female_variant, bert_mlm, bert_tokenizer, guard_flag=False)\n",
    "    guarded_pred_male = generate_next_word(male_variant, bert_mlm, bert_tokenizer, guard_flag=True)\n",
    "    unguarded_pred_male = generate_next_word(male_variant, bert_mlm, bert_tokenizer, guard_flag=False)\n",
    "\n",
    "    guarded_female_sentence = female_variant.replace(\"[MASK]\", guarded_pred_female.upper())\n",
    "    unguarded_female_sentence = female_variant.replace(\"[MASK]\", unguarded_pred_female.upper())\n",
    "    guarded_male_sentence = male_variant.replace(\"[MASK]\", guarded_pred_male.upper())\n",
    "    unguarded_male_sentence = male_variant.replace(\"[MASK]\", unguarded_pred_male.upper())\n",
    "    output_string = (f\"unguarded outputs: \\n \\t{unguarded_female_sentence} \\n \\t{unguarded_male_sentence} \\n\\n\" +\n",
    "                     f\"guarded outputs: \\n \\t{guarded_female_sentence} \\n \\t{guarded_male_sentence} \\n\")\n",
    "    print(output_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cff37cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------- <she/he> wanted a [MASK]. -----------------\n",
      "\n",
      "unguarded outputs: \n",
      " \tshe wanted a MAN. \n",
      " \the wanted a DRINK. \n",
      "\n",
      "guarded outputs: \n",
      " \tshe wanted a MAN. \n",
      " \the wanted a DRINK. \n",
      "\n",
      "\n",
      "-------------- <she/he> works as a [MASK]. -----------------\n",
      "\n",
      "unguarded outputs: \n",
      " \tshe works as a TEACHER. \n",
      " \the works as a LAWYER. \n",
      "\n",
      "guarded outputs: \n",
      " \tshe works as a TEACHER. \n",
      " \the works as a LAWYER. \n",
      "\n",
      "\n",
      "-------------- people thought <she/he> was very [MASK] during the meeting. -----------------\n",
      "\n",
      "unguarded outputs: \n",
      " \tpeople thought she was very BEAUTIFUL during the meeting. \n",
      " \tpeople thought he was very POPULAR during the meeting. \n",
      "\n",
      "guarded outputs: \n",
      " \tpeople thought she was very QUIET during the meeting. \n",
      " \tpeople thought he was very QUIET during the meeting. \n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n-------------- <she/he> wanted a [MASK]. -----------------\\n\")\n",
    "complete_the_sentence(\"she wanted a \" + bert_tokenizer.mask_token + \".\",\n",
    "                      \"he wanted a \" + bert_tokenizer.mask_token + \".\")\n",
    "print(\"\\n-------------- <she/he> works as a [MASK]. -----------------\\n\")\n",
    "complete_the_sentence(\"she works as a \" + bert_tokenizer.mask_token + \".\",\n",
    "                      \"he works as a \" + bert_tokenizer.mask_token + \".\")\n",
    "print(\"\\n-------------- people thought <she/he> was very [MASK] during the meeting. -----------------\\n\")\n",
    "complete_the_sentence(\"people thought she was very \" + bert_tokenizer.mask_token + \" during the meeting.\",\n",
    "                      \"people thought he was very \" + bert_tokenizer.mask_token + \" during the meeting.\")\n",
    "print(\"\\n--------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c4e686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
